# 🚖 Day 4 — Real-Time Tracking API & TTL/Privacy Strategy

## 🎯 Objective
Design a **low-latency, privacy-preserving** real-time tracking layer for a ride-hailing platform (Bimride). Provide:
- Bi-directional updates (driver → server → rider)
- **WebSockets** as primary transport, **REST** as fallback
- **<500 ms** end-to-end latency target
- Strict **TTL**, **scope-based access**, and **data minimization**

---

## 🧩 Tracking Architecture (End-to-End)

```
[Driver App]
└─(WS ping every 3–5s; burst on motion)─▶ [WS Gateway/API]
│
├─▶ Validate (JWT scope: driver)
├─▶ Enrich (tripId, geo-hash cell)
├─▶ Write-through cache: Redis (TTL 30–60s)
├─▶ Append to Kafka topic: location.trips (optional)
└─▶ Pub/Sub to rider channel (tripId)
│
[Rider App] ◀─(WS subscribe tripId)─────────────────────┘
▲
└─(REST fallback GET /tracking/trips/:tripId/location)
```

- **Hot path**: Driver pings → Redis upsert → WS broadcast to subscribed riders.
- **Cold path** (optional): Append to Kafka for analytics/replay (not on critical path).
- **Geo-hash sharding**: Route connections by geo cell for horizontal scale.

---

## 🔌 WebSocket Protocol

### 1) Authentication & Subscribe
- **Headers**: `Authorization: Bearer <JWT>`
- **Scopes**: `driver:publish_location`, `rider:subscribe_trip:<tripId>`
- On connect, client sends:

```json
{ "type": "SUBSCRIBE", "tripId": "t_456" }
```

Server validates rider ↔ trip relationship and joins channel `trip:t_456`.

### 2) Driver Location Ping

```json
{
  "type": "LOCATION_PING",
  "tripId": "t_456",
  "driverId": "d_777",
  "ts": "2025-08-14T14:05:21Z",
  "coords": { 
    "lat": 13.0971, 
    "lng": -59.6143, 
    "accuracy": 8.5, 
    "speed": 10.2, 
    "heading": 72 
  }
}
```

**Server action:**
- Validate trip ownership & state (EN_ROUTE or AT_PICKUP)
- Upsert Redis: key `loc:trip:t_456` → value (compact JSON) with TTL 60s
- Broadcast to channel `trip:t_456` (throttled to 1–2/s to riders)

### 3) Rider Update Payload

```json
{
  "type": "LOCATION_UPDATE",
  "tripId": "t_456",
  "ts": "2025-08-14T14:05:21Z",
  "driver": {
    "lat": 13.0971, 
    "lng": -59.6143, 
    "heading": 72, 
    "speed": 10.2
  },
  "etaSeconds": 240
}
```

*Note: No PII, no full path. Data minimization by default.*

---

## 🛟 REST Fallback Endpoints

### GET /v1/tracking/trips/{tripId}/location
Returns last known location from Redis (requires rider/driver scope for this trip).

### POST /v1/tracking/drivers/{driverId}/ping
Temporary fallback when WS unavailable (authenticated driver only).
Body mirrors LOCATION_PING. Same write-through logic → Redis + broadcast.

---

## 🗄️ Storage & TTL Model

### Redis Keys
- `loc:trip:{tripId}` → last known point, TTL 30–60s
- `poly:trip:{tripId}` → optional tiny polyline (generalized), TTL 10–15m (if enabled)
- `chan:trip:{tripId}` → subscriber set (managed by WS layer)

No long-term persistence in the hot layer. Historical paths belong in analytics lake (Kafka → S3/BigQuery) with anonymization.

### Why TTL?
- Auto-expires stale locations if driver goes offline
- Enforces post-trip invisibility (no live location after end + grace window)

---

## 🔐 Privacy, Scope & Compliance

### Scopes
- **Rider**: `rider:subscribe_trip:{tripId}`
- **Driver**: `driver:publish_location:{tripId}`
- **Admin**: read only for incident response, with audit trail

### Trip-Bound Access
- Only the rider (and co-riders) on that trip can subscribe.
- Location TTL expires ≤ 30 minutes after COMPLETED or CANCELED.

### Data Minimization
- Omit PII; send lat/lng + speed/heading only.
- Snap to roads (optional) before broadcast for UX, but store raw in analytics only if justified.

### Audit
- Subscription & admin reads logged to `tracking_audit` (userId, tripId, ts, ip, action).

---

## 🧮 Rate, Throttle & Backpressure

- **Driver → Server** ingestion: accept 1 ping per 3–5s; drop/coalesce bursts.
- **Server → Rider** broadcast: throttle to ≤ 2 Hz per trip to save data & battery.
- **WS Backpressure**: if rider client is slow, switch that client to REST polling.

---

## 🧪 Node.js (Express + ws) – Pseudocode

```typescript
// auth & scopes elided for brevity
wss.on('connection', (socket, ctx) => {
  const user = ctx.authUser; // from JWT
  socket.on('message', async (raw) => {
    const msg = JSON.parse(raw.toString());

    if (msg.type === 'SUBSCRIBE') {
      assertCanViewTrip(user, msg.tripId);
      joinChannel(socket, `trip:${msg.tripId}`);
      socket.send(JSON.stringify({ type: 'SUBSCRIBED', tripId: msg.tripId }));
      return;
    }

    if (msg.type === 'LOCATION_PING') {
      assertCanPublishForTrip(user, msg.tripId);
      const key = `loc:trip:${msg.tripId}`;
      const payload = compactPing(msg); // strip PII, round coords if needed
      await redis.setEx(key, 60, JSON.stringify(payload)); // TTL 60s
      // Throttled broadcast (per trip)
      broadcaster.enqueue(`trip:${msg.tripId}`, {
        type: 'LOCATION_UPDATE',
        tripId: msg.tripId,
        ts: payload.ts,
        driver: payload.coords,
        etaSeconds: estimateETA(payload) // optional server-side
      });
      return;
    }
  });
});

// REST fallback
app.get('/v1/tracking/trips/:tripId/location', authRiderOrDriver, async (req, res) => {
  assertCanViewTrip(req.user, req.params.tripId);
  const data = await redis.get(`loc:trip:${req.params.tripId}`);
  if (!data) return res.status(404).json({ error: 'NOT_FOUND' });
  return res.json(JSON.parse(data));
});
```

---

## 📏 Performance Targets

| Metric | Target |
|--------|--------|
| Ingest latency | < 150 ms (driver → server) |
| Fan-out latency | < 200 ms (server → rider) |
| End-to-end (p50) | < 350 ms |
| End-to-end (p95) | < 500 ms |
| WS reconnect time | < 2 s |

---

## 🧠 Edge Cases & Resilience

- **Network drops**: buffer 1–3 last pings on device; flush on reconnect.
- **App backgrounded**: switch to OS-optimized background location (coarser updates).
- **OS kills app**: background task APIs (iOS BGTask, Android WorkManager) push last known.
- **Clock skew**: trust server timestamps; treat client ts as advisory.
- **Multiple rider devices**: multi-subscriber fan-out per tripId.

---

## 🔍 Observability

### Metrics
- WS connections, subscriptions per trip
- Ingest & broadcast latency histograms
- Dropped/throttled messages count

### Logs
- Per trip channel lifecycle (subscribe/unsubscribe)
- Auth failures & permission denials

### Tracing
- Correlate driver ping → broadcast → rider receipt (OpenTelemetry)

---

## 🧭 Security Checklist

- TLS everywhere; HSTS at edge
- JWT + short-lived WS tokens; periodic rotation
- CORS/CSRF not relevant for WS but enforce origin allowlist for REST
- No location after TTL; purge caches on trip end
- Admin read access logged and reviewed

---

## 📌 Deliverables for Day 4

✅ WebSocket protocol (subscribe, ping, update) with scopes  
✅ REST fallback endpoints (+ same authorization rules)  
✅ Redis TTL model & data minimization policy  
✅ Throttling, backpressure & reconnect strategy  
✅ Performance & observability targets documented